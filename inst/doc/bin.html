<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Ramin Mojab" />

<meta name="date" content="2023-07-04" />

<title>Binary Regression</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Binary Regression</h1>
<h4 class="author">Ramin Mojab</h4>
<h4 class="date">2023-07-04</h4>



<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(ldt) </span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>seed <span class="ot">&lt;-</span> <span class="dv">123</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="fu">set.seed</span>(seed)</span></code></pre></div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this vignette, I will introduce you to the <code>ldt</code>
package and its main features for dealing with Logistic and Probit
Regression models. You will learn how to estimate a binary model, make
predictions, and assess model uncertainty. Additionally, we will explore
the use of Principal Component Analysis as an alternative approach for
handling a large number of potential explanatory variables.</p>
<p>One of the key ideas behind <code>ldt</code> is to minimize user
discretion by using a rule-based approach to select data. This approach
not only avoids discretion but also automates the process of searching
for the best models within a defined model set.</p>
<p>To demonstrate these features, I will create an artificial dataset
with a dependent variable and both relevant and irrelevant explanatory
variables. The dependent and relevant explanatory variables are sampled
from a known binary model. While we can evaluate how well the estimation
process finds the true parameters, our main focus will be on how to
estimate, search, predict, and report results.</p>
<p>Let’s get started!</p>
</div>
<div id="a-simple-experiment" class="section level2">
<h2>A simple experiment</h2>
<p>Let’s start by assuming that we know the structure of the model. We
can do this by simulating data from two known binary models. The
following command generates the required samples:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>num_obs <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>num_exo <span class="ot">&lt;-</span> 4L </span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>p_positive <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>max_weight <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>sample_l <span class="ot">&lt;-</span> <span class="fu">sim.bin</span>(num_exo, num_obs, <span class="at">probit =</span> <span class="cn">FALSE</span>, <span class="at">pPos =</span> p_positive, <span class="at">maxWeight =</span> max_weight)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>sample_p <span class="ot">&lt;-</span> <span class="fu">sim.bin</span>(sample_l<span class="sc">$</span>coef, num_obs, <span class="at">probit =</span> <span class="cn">TRUE</span>, <span class="at">pPos =</span> p_positive, <span class="at">maxWeight =</span> max_weight)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a> </span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="fu">print</span>(sample_l<span class="sc">$</span>coef)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>  <span class="sc">&gt;</span> [<span class="dv">1</span>] <span class="sc">-</span><span class="fl">0.56047565</span> <span class="sc">-</span><span class="fl">0.23017749</span>  <span class="fl">1.55870831</span>  <span class="fl">0.07050839</span></span></code></pre></div>
<p>We know the parameters of the systems because they are included in
the output of the <code>sim.bin</code> function. Note that there is a
logit and a probit model. Each model has one equation or dependent
variable. This equation has an intercept and 3 exogenous variables. The
sample size is 100. The coefficient vector of the logit model is
generated randomly and is listed in the output, <code>sample_l</code>.
It is used to sample data from the logit model and therefore, the
parameters of the two models are the same. In these samples, 40 percent
of observations are labeled as positive (because of <code>pPos</code>
argument). Finally, since <code>max_weight</code> is larger than 1,
observations are weighted and there is a w element in the returned lists
that we should consider in the estimation process.</p>
<p>The LaTeX code for the equation of the two models is included in the
<code>eqLatex</code> elements of the output. It results in the following
representations:</p>
Logit:
<span class="math display">\[\begin{aligned} P(Y = 1 | X_2, X_3, X_4) =
\frac{1}{1 + e^{-(-0.56 - 0.23 X_2 + 1.56 X_3 + 0.07 X_4)}}
\end{aligned}\]</span>
Probit:
<span class="math display">\[\begin{aligned} P(Y = 1 | X_2, X_3, X_4) =
\Phi(-0.56 - 0.23 X_2 + 1.56 X_3 + 0.07 X_4) \end{aligned}\]</span>
<p>Remember that these are the parameters of the system. We can use the
<code>glm</code> function to estimate them. The following code shows how
to do this. In the first line we prepare the equations and then we fit
two models for our two samples:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>eq_str <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;Y ~ &quot;</span>, <span class="fu">paste0</span>(<span class="fu">colnames</span>(sample_l<span class="sc">$</span>x[,<span class="sc">-</span><span class="dv">1</span>]), <span class="at">collapse =</span> <span class="st">&quot; + &quot;</span>))</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>fit_l <span class="ot">&lt;-</span> <span class="fu">glm</span>(eq_str, <span class="at">data =</span> <span class="fu">data.frame</span>(sample_l<span class="sc">$</span>y, sample_l<span class="sc">$</span>x), </span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>             <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>), <span class="at">weights =</span> sample_l<span class="sc">$</span>w) </span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>fit_p <span class="ot">&lt;-</span> <span class="fu">glm</span>(eq_str, <span class="at">data =</span> <span class="fu">data.frame</span>(sample_p<span class="sc">$</span>y, sample_p<span class="sc">$</span>x), </span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>             <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;probit&quot;</span>), <span class="at">weights =</span> sample_l<span class="sc">$</span>w) </span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a> </span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>fit_sf_l <span class="ot">&lt;-</span> <span class="fu">sim.bin</span>(fit_l<span class="sc">$</span>coefficients, <span class="at">probit =</span> <span class="cn">FALSE</span> )</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>fit_sf_p <span class="ot">&lt;-</span> <span class="fu">sim.bin</span>(fit_p<span class="sc">$</span>coefficients, <span class="at">probit =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>The last two lines are used for reporting the LaTeX formula. We use
eqLatex element of the output and this is the result:</p>
Logit:
<span class="math display">\[\begin{aligned} P(Y = 1 | X_2, X_3, X_4) =
\frac{1}{1 + e^{-(-0.46 - 0.36 X_2 + 1.82 X_3 + 0.26 X_4)}}
\end{aligned}\]</span>
Probit:
<span class="math display">\[\begin{aligned} P(Y = 1 | X_2, X_3, X_4) =
\Phi(-0.76 - 0.59 X_2 + 2.39 X_3 + 0.37 X_4) \end{aligned}\]</span>
<p>Note that these representations are not very appropriate for an
estimated model because they do not report the estimated standard errors
and there are better ways to do so. However, it suits our purpose here.
Also, note that you can get better results (in terms of lower difference
between actual and estimated coefficients) by increasing the sample
size.</p>
<p>The following code does the same by using the <code>estim.bin</code>
function in the <code>ldt</code> package:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>fit_l <span class="ot">&lt;-</span> <span class="fu">estim.bin</span>(sample_l<span class="sc">$</span>y, sample_l<span class="sc">$</span>x[,<span class="sc">-</span><span class="dv">1</span>], sample_l<span class="sc">$</span>w) </span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>fit_p <span class="ot">&lt;-</span> <span class="fu">estim.bin</span>(sample_p<span class="sc">$</span>y, sample_p<span class="sc">$</span>x[,<span class="sc">-</span><span class="dv">1</span>], sample_p<span class="sc">$</span>w, <span class="at">linkFunc =</span> <span class="st">&quot;probit&quot;</span>) </span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>res_table <span class="ot">&lt;-</span> <span class="fu">coefs.table</span>(<span class="fu">list</span>(<span class="at">Logit =</span> fit_l, <span class="at">Probit =</span> fit_p), </span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>                             <span class="at">regInfo =</span> <span class="fu">c</span>(<span class="st">&quot;obs&quot;</span>), <span class="at">formatLatex =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>The last line converts the estimated result into a table for
presentation. Additional arguments can be used to control the format and
level of information displayed in the table. The <code>kable</code>
function can be used to report the contents of the table. This is the
result:</p>
<table style="width:50%;border-bottom: 1px solid;">
<caption>
Results of estimation using <code>ldt::estim.bin</code> function.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Logit
</th>
<th style="text-align:left;">
Probit
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
-0.46<sup>**</sup>
</td>
<td style="text-align:left;">
-0.82<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left;">
X1
</td>
<td style="text-align:left;">
-0.36<sup></sup>
</td>
<td style="text-align:left;">
-0.47<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left;">
X2
</td>
<td style="text-align:left;">
1.82<sup>***</sup>
</td>
<td style="text-align:left;">
2.17<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left;">
X3
</td>
<td style="text-align:left;">
0.26<sup></sup>
</td>
<td style="text-align:left;">
0.32<sup>**</sup>
</td>
</tr>
<tr>
<td style="text-align:left;">
obs
</td>
<td style="text-align:left;">
100
</td>
<td style="text-align:left;">
100
</td>
</tr>
</tbody>
</table>
<p>Differences between the results of <code>systemfit</code> and
<code>estim.bin</code> functions may be due to variations in
initialization or optimization procedures.</p>
<p>While the <code>coefs.plot</code> function was discussed in <a href="sur.html">another vignette on SUR models</a>, this vignette
focuses on prediction rather than parameter estimation. As such, it will
not be covered here.</p>
</div>
<div id="prediction" class="section level2">
<h2>Prediction</h2>
<p>A binary regression model can be used in binary classification
practices. It involves using the model to predict the likelihood that a
new case will fall into one of two classes based on the values of the
explanatory variables. The class is either positive (<span class="math inline">\(Y = 1\)</span>) or negative (<span class="math inline">\(Y = 0\)</span>). The regression model estimates
the likelihood given the explanatory variables, i.e., <span class="math inline">\(P(Y=1∣X)\)</span>. We can calculate the other
probability: <span class="math inline">\(P(Y=0∣X)=1−P(Y=1∣X)\)</span>.
Deciding the class of the observation given the probability need a
decision rule.</p>
<p>To continue our experiment, let’s focus on the logit model and
generate a sample of size 10 from the true model using the
<code>sim.bin</code> function. We can predict using the
<code>estim.bin</code> function in <code>ldt</code> by setting the
<code>newX</code> argument. This is the code:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>sample_l_new <span class="ot">&lt;-</span> <span class="fu">sim.bin</span>(sample_l<span class="sc">$</span>coef, <span class="dv">10</span>, <span class="at">probit =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>fit_l <span class="ot">&lt;-</span> <span class="fu">estim.bin</span>(sample_l<span class="sc">$</span>y, sample_l<span class="sc">$</span>x[,<span class="sc">-</span><span class="dv">1</span>], sample_l<span class="sc">$</span>w, <span class="at">newX =</span> sample_l_new<span class="sc">$</span>x[,<span class="sc">-</span><span class="dv">1</span>])</span></code></pre></div>
<p>We have three types of information: the actual class of the
observation (<code>sample_l$y</code>), the probabilities used in the
simulation process (<code>sample_l$p1</code>), and the predicted
probabilities <code>fit_l$projection[,2]</code>. Let’s plot them:</p>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>As you can see, it’s possible to have positive observations with low
probabilities and negative observations with high probabilities due to
the random nature of the binomial distribution.</p>
<p>In practice, we do not see the green circles, i.e., <span class="math inline">\(P(Y=1∣X)\)</span>. We only see the actual class of
observations. The binary regression model predicts the green circles.
This means that even if we have the exact coefficient vector, the best
we can do is to predict the exact probability and not the actual class
of observation. There must be a decision-making process that converts
the predicted probability to a specific class.</p>
<p>When using a binary regression model to make classification
decisions, a common approach is to define a threshold value for the
predicted probability. If the predicted probability for a case is
greater than or equal to the threshold value, the case is classified as
belonging to the positive class. If the predicted probability is less
than the threshold value, the case is classified as belonging to the
negative class.</p>
<p>However, threshold is not a parameter of the model but just a
decision rule. The choice of threshold value can affect the balance
between sensitivity and specificity but cannot completely eliminate
classification error. One might design a more complex decision-making
process than using just one fixed threshold level. There are several
ways to evaluate the predictive power of a binary regression model. One
common approach is to use a confusion matrix to calculate accuracy,
sensitivity, specificity, and precision. Other metrics include AUC-ROC
and Brier score. The AUC-ROC represents the probability that a randomly
chosen positive case will have a higher predicted probability than a
randomly chosen negative case. A lower Brier score indicates better
predictive performance.</p>
<p>Similar to the previous subsection, we can use
<code>coefs.table</code> function and report the results in a table:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>res_table <span class="ot">&lt;-</span> <span class="fu">coefs.table</span>(<span class="fu">list</span>(<span class="at">Logit =</span> fit_l), </span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>                             <span class="at">regInfo =</span> <span class="fu">c</span>(<span class="st">&quot;obs&quot;</span>, <span class="st">&quot;aic&quot;</span>, <span class="st">&quot;sic&quot;</span>, <span class="st">&quot;aucIn&quot;</span>, <span class="st">&quot;brierIn&quot;</span>), </span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>                             <span class="at">formatLatex =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<table style="width:50%;border-bottom: 1px solid;">
<caption>
Results of estimation using <code>ldt::estim.bin</code> function with
prediction.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
-0.46<sup>**</sup>
</td>
</tr>
<tr>
<td style="text-align:left;">
X1
</td>
<td style="text-align:left;">
-0.36<sup></sup>
</td>
</tr>
<tr>
<td style="text-align:left;">
X2
</td>
<td style="text-align:left;">
1.82<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left;">
X3
</td>
<td style="text-align:left;">
0.26<sup></sup>
</td>
</tr>
<tr>
<td style="text-align:left;">
obs
</td>
<td style="text-align:left;">
100
</td>
</tr>
<tr>
<td style="text-align:left;">
aic
</td>
<td style="text-align:left;">
161.12
</td>
</tr>
<tr>
<td style="text-align:left;">
sic
</td>
<td style="text-align:left;">
171.55
</td>
</tr>
<tr>
<td style="text-align:left;">
aucIn
</td>
<td style="text-align:left;">
0.83
</td>
</tr>
<tr>
<td style="text-align:left;">
brierIn
</td>
<td style="text-align:left;">
0.16
</td>
</tr>
</tbody>
</table>
<p>One metric (specific to our experiment) is not present in the table.
Its formula is <span class="math inline">\(a =
\frac{\sum_{i=1}^{n}(p_i-\hat{p}_i)^2}{n}\)</span> where <span class="math inline">\(p_i\)</span> represents the actual probabilities
(the green circles in the plot) and <span class="math inline">\(\hat{p}_i\)</span> represents the predicted
probabilities (the red plus signs in the plot). This metric is similar
to the Brier score but uses actual probabilities. A value of zero
indicates a perfect fit and larger values indicate lower explanatory
power. The value for this experiment is 0.0035284.</p>
</div>
<div id="model-uncertainty" class="section level2">
<h2>Model uncertainty</h2>
<p>Let’s consider a more realistic situation where model uncertainty
exists. That’s where <code>ldt</code> can help. In the previous
subsection, we knew all relevant explanatory variables. Here, we
consider a situation where there are some irrelevant variables too. We
limit the level of uncertainty and other practical issues by restricting
the number of these variables. The following code reflects our
assumptions:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>sample_l<span class="sc">$</span>x <span class="ot">&lt;-</span> <span class="fu">cbind</span>(sample_l<span class="sc">$</span>x, <span class="fu">matrix</span>(<span class="fu">rnorm</span>(num_obs <span class="sc">*</span> <span class="dv">50</span>), <span class="at">ncol =</span> <span class="dv">50</span>, </span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>                                   <span class="at">dimnames =</span> <span class="fu">list</span>(<span class="cn">NULL</span>,<span class="fu">paste0</span>(<span class="st">&quot;z&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>))))</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>sample_l_new<span class="sc">$</span>x <span class="ot">&lt;-</span> <span class="fu">cbind</span>(sample_l_new<span class="sc">$</span>x, <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="fu">nrow</span>(sample_l_new<span class="sc">$</span>x) <span class="sc">*</span> <span class="dv">50</span>), <span class="at">ncol =</span> <span class="dv">50</span>, </span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>                                   <span class="at">dimnames =</span> <span class="fu">list</span>(<span class="cn">NULL</span>,<span class="fu">paste0</span>(<span class="st">&quot;z&quot;</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>))))</span></code></pre></div>
<p>In our experiment, there are 50 irrelevant and 3 relevant variables.
The number of irrelevant data is relatively large and their names start
with the <code>z</code> character. The second line of code creates
out-of-sample data in the extended sample for use in prediction.</p>
<p>The following code uses the <code>search.bin</code> function to find
the actual model:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>search_res <span class="ot">&lt;-</span> <span class="fu">search.bin</span>(sample_l<span class="sc">$</span>y, sample_l<span class="sc">$</span>x[,<span class="sc">-</span><span class="dv">1</span>], sample_l<span class="sc">$</span>w,</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>                        <span class="at">xSizes =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>), </span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>                        <span class="at">metricOptions =</span> <span class="fu">get.options.metric</span>(<span class="at">typesIn =</span> <span class="fu">c</span>(<span class="st">&quot;sic&quot;</span>)))</span></code></pre></div>
<p>The <code>xSizes = c(1:4)</code> part assumes that we know the number
of relevant explanatory variables is less than 5. The
<code>metric_options</code> part shows that we use SIC metrics to
evaluate and compare models.</p>
<p>This code is time-consuming and is not evaluated here. However, on my
system, the elapsed time is 77 seconds (the number of searched models is
317682). Note that if we change our previous guess and assume that the
maximum number of relevant variables is larger, for example 5, the size
of the practical model set becomes 3187367 (10 times larger) and this is
estimated in 977 seconds (12 times larger) on my system. Many factors
affect this time, including optimization process options. Also check the
parallel option in <code>get.options.search()</code> function.</p>
<p>One might reduce the number of potential explanatory variables using
theory or statistical testing. Since <code>ldt</code> dislikes user
discretion, it provides a more systematic approach. The idea behind it
is simple: estimate smaller models, select variables, estimate larger
models with fewer potential explanatory variables. Here is the code:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>x_size_steps <span class="ot">=</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="fu">c</span>(<span class="dv">3</span>), <span class="fu">c</span>(<span class="dv">4</span>), <span class="fu">c</span>(<span class="dv">5</span>))</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>count_steps <span class="ot">=</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">9</span>)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>search_step_res <span class="ot">&lt;-</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>  <span class="fu">search.bin.stepwise</span>(<span class="at">y =</span> sample_l<span class="sc">$</span>y, <span class="at">x =</span> sample_l<span class="sc">$</span>x, <span class="at">w =</span> sample_l<span class="sc">$</span>w,</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>                      <span class="at">xSizeSteps =</span> x_size_steps, <span class="at">countSteps =</span> count_steps,</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>                      <span class="at">metricOptions =</span> <span class="fu">get.options.metric</span>(<span class="at">typesIn =</span> <span class="fu">c</span>(<span class="st">&quot;aic&quot;</span>,<span class="st">&quot;sic&quot;</span>, <span class="st">&quot;auc&quot;</span>, <span class="st">&quot;brier&quot;</span>)),</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>                      <span class="at">searchItems =</span> <span class="fu">get.items.search</span>(<span class="at">bestK =</span> <span class="dv">10</span>))</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>  <span class="sc">&gt;</span> Warning <span class="cf">in</span> <span class="fu">.SearchDc</span>(y, x, w, xSizes, xPartitions, costMatrices, searchLogit, <span class="sc">:</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>  <span class="er">&gt;</span> Error occurred <span class="cf">in</span> the search process. See <span class="st">&#39;result$counts&#39;</span>.</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>search_step_res</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>  <span class="sc">&gt;</span> method<span class="sc">:</span> bin </span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>  <span class="sc">&gt;</span> expected<span class="sc">:</span> <span class="dv">2</span>,<span class="dv">763</span>, searched<span class="sc">:</span> <span class="dv">2</span>,<span class="dv">763</span> (<span class="dv">100</span><span class="sc">%), failed: 54 (2%</span>)</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>  <span class="sc">&gt;</span> elapsed time<span class="sc">:</span> <span class="fl">0.06699518</span> minutes </span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>  <span class="sc">&gt;</span> <span class="sc">--------</span></span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>  <span class="er">&gt;</span> Failures<span class="sc">:</span></span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>  <span class="er">&gt;</span> <span class="fl">1.</span> matrix singularity<span class="sc">:</span> <span class="dv">54</span> (<span class="dv">100</span>%)</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>  <span class="sc">&gt;</span> <span class="sc">--------</span></span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>  <span class="er">&gt;</span> <span class="fl">1.</span> aic<span class="sc">:</span></span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>  <span class="er">&gt;</span>  <span class="fu">Y</span> (<span class="at">best=</span><span class="fl">131.239</span>)</span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>  <span class="sc">&gt;</span> <span class="fl">2.</span> sic<span class="sc">:</span></span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>  <span class="er">&gt;</span>  <span class="fu">Y</span> (<span class="at">best=</span><span class="fl">146.87</span>)</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>  <span class="sc">&gt;</span> <span class="fl">3.</span> aucIn<span class="sc">:</span></span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>  <span class="er">&gt;</span>  <span class="fu">Y</span> (<span class="at">best=</span><span class="fl">0.896</span>)</span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a>  <span class="sc">&gt;</span> <span class="fl">4.</span> brierIn<span class="sc">:</span></span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a>  <span class="er">&gt;</span>  <span class="fu">Y</span> (<span class="at">best=</span><span class="fl">0.122</span>)</span></code></pre></div>
<p>The first two lines define the steps. We use all variables
(<code>NA</code> in <code>count_steps</code> means all) to estimate
models with sizes defined as the first element of
<code>x_size_steps</code>. Then we select a number of variables from the
information provided by the best models and estimate models with sizes
determined by the second element of <code>x_size_steps</code>. And so
on.</p>
<p>The size of the model subset and running time are greatly reduced.
However, let’s see its performance.</p>
<p>To study or report results, we should use the <code>summary</code>
function. The output of a search project in <code>ldt</code> does not
contain estimation results but only the minimum level of information to
replicate them. The <code>summary</code> function does the job and
estimates the models. Here is the code:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>ssum <span class="ot">&lt;-</span> <span class="fu">summary</span>(search_step_res, </span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>                <span class="at">y =</span> sample_l<span class="sc">$</span>y, <span class="at">x =</span> sample_l<span class="sc">$</span>x, <span class="at">w =</span> sample_l<span class="sc">$</span>w, </span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>                <span class="at">newX =</span> sample_l_new<span class="sc">$</span>x, </span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>                <span class="at">printMsg =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Usually, there is more than one model in the <code>summary</code>
output. This is because the output is first “target-variable-specific”
and second “evaluation-specific”. In this application, there is just one
target but we requested four different types of evaluations in the
get.options.metric function. We can report the results by creating a
list of estimated models and using the coefs.table function:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>mod_list <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="st">&quot;SIC&quot;</span> <span class="ot">=</span> ssum<span class="sc">$</span>sic<span class="sc">$</span>target1<span class="sc">$</span>model<span class="sc">$</span>bests<span class="sc">$</span>best1,</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>                 <span class="st">&quot;AIC&quot;</span> <span class="ot">=</span> ssum<span class="sc">$</span>aic<span class="sc">$</span>target1<span class="sc">$</span>model<span class="sc">$</span>bests<span class="sc">$</span>best1,</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>                 <span class="st">&quot;AUC&quot;</span> <span class="ot">=</span> ssum<span class="sc">$</span>aucIn<span class="sc">$</span>target1<span class="sc">$</span>model<span class="sc">$</span>bests<span class="sc">$</span>best1,</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>                 <span class="st">&quot;Brier&quot;</span> <span class="ot">=</span> ssum<span class="sc">$</span>brierIn<span class="sc">$</span>target1<span class="sc">$</span>model<span class="sc">$</span>bests<span class="sc">$</span>best1)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>res_table <span class="ot">&lt;-</span> <span class="fu">coefs.table</span>(mod_list, </span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>                             <span class="at">regInfo =</span> <span class="fu">c</span>(<span class="st">&quot;obs&quot;</span>, <span class="st">&quot;aic&quot;</span>, <span class="st">&quot;sic&quot;</span>, <span class="st">&quot;aucIn&quot;</span>, <span class="st">&quot;brierIn&quot;</span>), </span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>                             <span class="at">formatLatex =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Since we set the <code>newX</code> argument in the
<code>summary</code> function, we can plot the predictions:</p>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<p>With current seed and other options, all metrics point to a common
model. In this regression model a relevant variable (<code>X3</code>) is
missing and three irrelevant variables are present with non-significant
coefficients.</p>
</div>
<div id="out-of-sample-simulation" class="section level2">
<h2>Out-of-sample simulation</h2>
<p>Let’s see if we can do any better by using out-of-sample evaluations.
The following code is similar to the code in the previous section, but
we define a out-of-sample process:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>metric_options <span class="ot">&lt;-</span> <span class="fu">get.options.metric</span>(<span class="at">typesOut =</span> <span class="fu">c</span>(<span class="st">&quot;auc&quot;</span>, <span class="st">&quot;brier&quot;</span>), </span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>                                       <span class="at">seed =</span> <span class="sc">-</span>seed, <span class="at">simFixSize =</span> <span class="dv">5</span>, <span class="at">trainRatio =</span> <span class="fl">0.75</span>)</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>search_step_res <span class="ot">&lt;-</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>  <span class="fu">search.bin.stepwise</span>(<span class="at">y =</span> sample_l<span class="sc">$</span>y, <span class="at">x =</span> sample_l<span class="sc">$</span>x, <span class="at">w =</span> sample_l<span class="sc">$</span>w,</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>                      <span class="at">xSizeSteps =</span> x_size_steps, <span class="at">countSteps =</span> count_steps,</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>                      <span class="at">metricOptions =</span> metric_options,</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>                      <span class="at">searchItems =</span> <span class="fu">get.items.search</span>(<span class="at">bestK =</span> <span class="dv">10</span>),</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>                      <span class="at">searchOptions =</span> <span class="fu">get.options.search</span>(<span class="at">printMsg =</span> <span class="cn">FALSE</span>))</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>  <span class="sc">&gt;</span> Warning <span class="cf">in</span> <span class="fu">.SearchDc</span>(y, x, w, xSizes, xPartitions, costMatrices, searchLogit, <span class="sc">:</span></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>  <span class="er">&gt;</span> Error occurred <span class="cf">in</span> the search process. See <span class="st">&#39;result$counts&#39;</span>.</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>search_step_res</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>  <span class="sc">&gt;</span> method<span class="sc">:</span> bin </span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>  <span class="sc">&gt;</span> expected<span class="sc">:</span> <span class="dv">2</span>,<span class="dv">276</span>, searched<span class="sc">:</span> <span class="dv">2</span>,<span class="dv">276</span> (<span class="dv">100</span><span class="sc">%), failed: 54 (2.4%</span>)</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>  <span class="sc">&gt;</span> elapsed time<span class="sc">:</span> <span class="fl">0.06703867</span> minutes </span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>  <span class="sc">&gt;</span> <span class="sc">--------</span></span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>  <span class="er">&gt;</span> Failures<span class="sc">:</span></span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a>  <span class="er">&gt;</span> <span class="fl">1.</span> matrix singularity<span class="sc">:</span> <span class="dv">54</span> (<span class="dv">100</span>%)</span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>  <span class="sc">&gt;</span> <span class="sc">--------</span></span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a>  <span class="er">&gt;</span> <span class="fl">1.</span> aucOut<span class="sc">:</span></span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a>  <span class="er">&gt;</span>  <span class="fu">Y</span> (<span class="at">best=</span><span class="fl">0.931</span>)</span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a>  <span class="sc">&gt;</span> <span class="fl">2.</span> brierOut<span class="sc">:</span></span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a>  <span class="er">&gt;</span>  <span class="fu">Y</span> (<span class="at">best=</span><span class="fl">0.094</span>)</span></code></pre></div>
<p>We use 0.75 ratio of the observations (determined by
<code>trainRatio</code>) for estimating and the rest for testing. We
repeat this experiment 5 times (determined by <code>simFixSize</code>).
We can report the result similar to the previous discussion:</p>
<p>You can get better results by increasing the number of observations.
Also, you can change optimization algorithm options or other search
options to improve performance.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
